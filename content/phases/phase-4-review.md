# Phase 4: Multi-AI Review & Refinement

---

## Objective
Use multiple AI models and automated tools to review code for issues the original AI missed.

---

<a id="reviewer-independence"></a>
## The Reviewer Independence Rule ⚠️

**CRITICAL**: The AI model that reviews your code MUST be different from the one that generated it.

```
┌─────────────────────────────────────────────────────────┐
│  REVIEWER ≠ GENERATOR                                   │
│                                                         │
│  WHY: Same AI makes same mistakes twice                │
│       Same blind spots → Same bugs missed              │
│                                                         │
│  Code by Claude  → Review by Copilot/Cursor           │
│  Code by Copilot → Review by Claude/Antigravity       │
│  Code by Cursor  → Review by Claude/Copilot           │
└─────────────────────────────────────────────────────────┘
```

**Research shows**: Different AI models catch 40% more bugs than using the same model.

---

## Step 4.1: Select Review Tools

**Use at least 2 different review tools:**

**Minimum (All developers):**
1. Different AI model than generator
2. Static analysis (ESLint/Pylint/etc.)

**Recommended (After first month):**
1. Different AI model
2. Static analysis tool
3. Code quality tool (Sourcery/SonarLint)

**Advanced (Experienced teams):**
1. Multiple AI models
2. Static analysis
3. Code quality tool
4. Security scanner (Antigravity/Snyk)

---

## Step 4.2: AI Model Review

**Use a DIFFERENT AI model from the one that generated the code:**

```
PROMPT TO USE:
"Review this code (generated by [MODEL_NAME]):

[Paste code]

Constitution: [paste relevant sections]
Spec: [paste acceptance criteria]

Check for:
1. Constitution violations
2. Security issues (injection, auth, data exposure)
3. Performance problems (N+1 queries, memory leaks)
4. Missing error handling
5. Edge cases not tested
6. Code smells (duplication, complexity)

Generate detailed review report with:
- Issue severity (Critical/High/Medium/Low)
- Specific line numbers
- Suggested fixes"
```

**Track which models you used:**
```markdown
## AI Tool Chain
- Generated by: Claude Sonnet 4
- Reviewed by: GitHub Copilot, Sourcery
```

---

## Step 4.3: Run Automated Tools

**Run multiple automated review tools:**

```bash
# Linting (style & basic errors)
npm run lint              # Node.js
pylint src/               # Python
mvn checkstyle:check      # Java
dotnet format --verify    # .NET

# Code quality analysis
sourcery review .         # Multi-language
sonarlint analyze         # IDE-based

# Security scanning (if available)
antigravity review --security-only
snyk test

# Full static analysis (if configured)
sonar-scanner
deepsource analyze
```

---

## Step 4.4: Document Review Findings

**Save review feedback** in your specs directory:

```markdown
# Review Feedback - [Feature Name]

**Date**: [Date]
**Reviewers**: [AI models + tools used]

## Critical Issues
- [ ] [Issue 1 with severity, line numbers, fix]
- [ ] [Issue 2 with severity, line numbers, fix]

## High Priority
- [ ] [Issue 3]
- [ ] [Issue 4]

## Medium Priority
- [ ] [Issue 5]
- [ ] [Issue 6]

## Improvements (Nice to Have)
- [ ] [Suggestion 1]
- [ ] [Suggestion 2]

## What Went Well
- ✅ [Positive finding 1]
- ✅ [Positive finding 2]
```

---

## Step 4.5: Fix Critical Issues

**Address all Critical and High priority issues** before creating PR.

**Iterate**:
1. Fix issue
2. Re-run tests (must stay GREEN)
3. Re-run review tools
4. Verify fix doesn't introduce new issues

---

## Checkpoint 4 (GATE)

Before moving to Phase 5, verify:

- [ ] Different AI model reviewed code (not the same one that generated it)
- [ ] At least 2 automated tools ran
- [ ] All Critical issues fixed
- [ ] All High priority issues fixed or documented as accepted risk
- [ ] Tests still pass after fixes
- [ ] Coverage still >= 80%
- [ ] Review feedback documented

**If checkpoint fails**: Continue fixing issues. Don't create PR until critical issues resolved.
