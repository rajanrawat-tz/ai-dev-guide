# ðŸ“‹ AI Development Quick Reference Card

---

## ðŸ“ Phase 0: Creating Foundation Documents

### From PRD to Feature Specification
```
PRD â†’ Analyze Features â†’ Define Stories â†’ Write Acceptance Criteria â†’ Feature Specification.md
```

### From Feature Specification to Implementation Guide
```
Feature Specification â†’ Research Patterns â†’ Design Abstractions â†’ Code Examples â†’ Implementation Guide.md
```

---

## ðŸ› ï¸ Speckit Commands

| Command | When to Use | Junior-Friendly? |
|---------|-------------|------------------|
| `/speckit.constitution` | Create/update project rules | Advanced (ask senior) |
| `/speckit.specify` | Generate feature spec from story | **Start here** |
| `/speckit.clarify` | Ask clarifying questions about spec | **Use liberally** |
| `/speckit.plan` | Create implementation plan | After spec done |
| `/speckit.tasks` | Generate task breakdown | After plan approved |
| `/speckit.analyze` | Cross-check spec/plan/tasks | Before Phase 2 |
| `/speckit.checklist` | Generate requirements checklist | Optional validation |
| `/speckit.implement` | Execute implementation | Rarely used |
| `/speckit.taskstoissues` | Convert tasks to GitHub issues | Start of Phase 2 |

**No Speckit access?** Use manual templates in your project's templates directory and get senior review before implementing.

---

## ðŸ”„ Development Flow (5 Phases)

```
Phase 0         Phase 1          Phase 2              Phase 3          Phase 4
Foundation  â†’   Speckit      â†’   GitHub           â†’   Implement    â†’   Multi-Model
(PRDâ†’FSâ†’IG)     Artifacts        Issues (Hierarchy)   + TDD            Review

â±ï¸ 30-60m      â±ï¸ 45-90m        â±ï¸ 15m              â±ï¸ 4-8h          â±ï¸ 1-2h

                                 Epic â†’ US â†’ Tasks
```

**First feature**: ~12 hours total | **After 5 features**: ~6 hours total

**Note**: FS = Feature Specification, IG = Implementation Guide

---

## ðŸ“š Core Documents

| Doc | Typical Location | Use For | Can I Skip? |
|-----|----------|---------|-------------|
| **PRD** | Project-specific | Requirements from stakeholders | Never |
| **Feature Specification** | Project root or docs/ | User stories, acceptance criteria | Never |
| **Implementation Guide** | Project root or docs/ | Patterns & code examples | After 5 features |
| **Constitution** | Project-specific (varies) | Technical constraints | Never |

**Note**: Ask your team lead for exact document locations in your project.

---

## âš ï¸ REVIEWER INDEPENDENCE RULE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CRITICAL: REVIEWER â‰  GENERATOR                         â”‚
â”‚                                                         â”‚
â”‚  WHY: Same AI makes same mistakes twice                â”‚
â”‚       Same blind spots â†’ Same bugs missed              â”‚
â”‚                                                         â”‚
â”‚  Code by Claude  â†’ Review by Copilot/Cursor           â”‚
â”‚  Code by Copilot â†’ Review by Claude/Antigravity       â”‚
â”‚  Code by Cursor  â†’ Review by Claude/Copilot           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Track your chain in PR description:**
```markdown
## AI Tool Chain
- Generated by: _________
- Reviewed by: _________, _________
```

**Why this matters**: Same AI has same blind spots. Different models catch 40% more bugs.

---

## ðŸ¤– AI Review Tools by Experience Level

### Month 1-2 (Start Simple)
- One AI coding assistant (pick Claude OR Copilot)
- ESLint/Prettier (style & standards)
- Different AI for review (if Claude codes, Copilot reviews)

### Month 3-6 (Add Static Analysis)
- Sourcery - Automated code quality
- SonarLint - Bug detection
- Keep using different AI for reviews

### Month 6+ (Full Toolkit)
- Antigravity - Architecture & security
- CodeRabbit - Auto PR review
- SonarQube - Full project analysis

### Tool Comparison

| Tool | Type | Best For | When to Use |
|------|------|----------|-------------|
| **Sourcery** | Static | Code quality, refactoring | Every PR |
| **ESLint** | Static | Style, standards | Every commit |
| **Antigravity** | AI | Architecture, security | Complex features |
| **Claude** | LLM | Deep analysis, constitution | Code generation & review |
| **Copilot** | LLM | Quick fixes, inline help | Code generation & review |
| **Cursor** | LLM IDE | Full-file review, editing | Code generation & review |
| **CodeRabbit** | AI | PR auto-review | Every PR (if installed) |
| **SonarQube** | Static | Bugs, vulnerabilities, debt | Weekly/release |
| **DeepSource** | Static | Multi-language, auto-fix | CI pipeline |

---

## ðŸ§ª TDD Workflow (MANDATORY)

```
1. WRITE TEST â†’ 2. RUN (FAIL) â†’ 3. IMPLEMENT â†’ 4. RUN (PASS) â†’ 5. REFACTOR
     ðŸ”´ RED                           ðŸŸ¢ GREEN          âœ¨ CLEAN
```

**NEVER write implementation before test!**

**Why TDD is mandatory:**
- AI code has hidden edge cases
- Tests prove code works before PR
- Prevents 80% of production bugs
- Refactoring is safe with test coverage

**Coverage requirement**: >= 80% (no exceptions)

---

## ðŸ“œ Constitution Quick Check

**Before implementing, verify against YOUR constitution:**

This is a template - your project's constitution will have specific technical principles.

**Example areas your constitution should cover:**
1. **Stack**: Your approved tech stack and versions
2. **Architecture**: Your patterns (microservices, monolith, etc.)
3. **TDD**: Tests FIRST, coverage minimum
4. **Data Handling**: Data retention, privacy, compliance
5. **Persistence**: Database strategy, caching approach
6. **APIs/Messaging**: REST/GraphQL/event patterns
7. **Config**: Configuration management approach
8. **Containers**: Docker/Kubernetes requirements
9. **Testing**: Unit, integration, e2e requirements
10. **Observability**: Logging, metrics, monitoring

**Don't memorize** - Check your constitution document every feature.
**Path varies by project** - Ask your team lead for location.

---

## âœ… Checkpoint Gates (Don't Skip These!)

### Phase 0 (Foundation)
- [ ] PRD understood
- [ ] Feature Specification exists (or created)
- [ ] Implementation Guide exists (or created)
- [ ] Constitution read

### Phase 1 (Specification)
- [ ] spec.md complete & clear
- [ ] plan.md has constitution check
- [ ] tasks.md has TDD tasks
- [ ] `/speckit.analyze` passes (or manual review complete)

### Phase 2 (GitHub Issues)
- [ ] Epic created (if applicable)
- [ ] User Story issue created
- [ ] All task issues created
- [ ] **Hierarchy set**: Epic â†’ US â†’ Tasks
- [ ] Feature branch created
- [ ] Issues assigned to team members

### Phase 3 (Implementation)
- [ ] All tests pass
- [ ] Coverage >= 80%
- [ ] Docker tests work
- [ ] Docs updated
- [ ] **quickstart.md created**
- [ ] Constitution compliant

### Phase 4 (Review)
- [ ] Different AI model reviewed code
- [ ] 2+ automated tools ran
- [ ] All critical issues fixed
- [ ] PR checklist complete

---

## ðŸ’¬ Common Prompts (Copy-Paste Ready)

### Research Phase
```
"Analyze the codebase for [feature] and create a pre-implementation document that covers:
- Related existing code
- Files to modify
- Technical decisions needed
- Risks and dependencies"
```

### Start New Feature
```
"I'm starting [STORY-ID]. Read constitution, implementation guide, feature specification.
Perform pre-implementation analysis and identify:
1. Files to create/modify
2. Dependencies
3. Technical decisions needed"
```

### TDD Implementation
```
"Implement [task] following TDD:
1. Write test FIRST at [test file path]
2. Show me test code
3. Run it (should FAIL/RED)
4. Implement code to make it pass
5. Refactor while keeping GREEN"
```

### Code Review (Different Model)
```
"Review this code (generated by [MODEL]):

Constitution: [paste relevant sections]
Spec: [paste acceptance criteria]

Check for:
1. Constitution violations
2. Security issues
3. Performance problems
4. Missing error handling
5. Edge cases not tested"
```

---

## ðŸ”§ Common Situations & Quick Actions

| Situation | Quick Action |
|-----------|--------------|
| **Bug in production** | Create issue â†’ Write failing test â†’ Fix â†’ PR |
| **Spec unclear** | `/speckit.clarify` â†’ Update spec â†’ Get confirmation |
| **Constitution blocks approach** | Document â†’ Discuss with lead â†’ ADR if needed |
| **AI code looks wrong** | **Trust your instincts** â†’ Check constitution â†’ Different AI review â†’ Ask senior |
| **Speckit down** | Use templates â†’ Manual spec â†’ Senior review before Phase 2 |
| **Tests take too long** | **Don't skip** â†’ Start with critical path â†’ Add edge cases incrementally |
| **Blocked by dependency** | Update issue â†’ Work on parallel tasks â†’ Create mocks for testing |
| **Working after hours** | Check constitution â†’ Use `/speckit.clarify` (async) â†’ Post in team chat â†’ Document blockers |

---

## ðŸ›‘ Emergency Stops

**STOP IMMEDIATELY IF:**
- Constitution check fails â†’ Escalate to tech lead
- Can't write test first â†’ Requirements unclear, `/speckit.clarify`
- Security issue found â†’ Fix before proceeding
- Coverage < 80% â†’ Write more tests
- Same AI reviewing its own code â†’ Use different model
- Docker tests fail â†’ Debug before PR

**ESCALATE TO:**
- Tech lead for architecture decisions
- Create ADR if constitution change needed
- Senior developer if stuck > 2 hours

---

## ðŸ’» Daily Commands

### Morning
```bash
git checkout main && git pull
gh issue list --assignee @me
```

### Start New Work
```bash
git checkout -b [feature-id]-[name]
```

### During Development
```bash
# Run tests frequently (adjust for your language/framework)
npm test [test-file]           # Node.js
pytest tests/[test-file]       # Python
mvn test -Dtest=[TestClass]    # Java
dotnet test --filter [test]    # .NET

# Check coverage
npm run test:coverage          # Node.js
pytest --cov                   # Python
mvn test jacoco:report         # Java
dotnet test /p:CollectCoverage=true  # .NET

# Docker testing
docker-compose up -d
npm run test:integration  # or your integration test command
docker-compose down
```

### End of Day
```bash
git add . && git commit -m "wip: [desc]"
git push
gh issue comment [#] -b "Progress: [what done]"
```

### Code Review
```bash
# Run at least 2 tools
npm run lint  # or eslint/pylint/etc
sourcery review .
# antigravity review . (if available)
```

### Create PR
```bash
gh pr create --title "feat([story-id]): [name]" \
  --body "[Include AI tool chain + checklist]"
```

---

## ðŸ“ File Structure

**Note**: Structure may vary by project. Common pattern:

```
specs/[feature-id]/
  spec.md           # What to build
  plan.md           # How to build (constitution check here)
  tasks.md          # Step by step (TDD workflow here)
  review-feedback.md # AI + tool review results
  research.md       # Why decisions (if needed)
  data-model.md     # Entities (if needed)
  contracts/        # APIs & events
  checklists/       # Validation
```

---

## â±ï¸ Time Expectations (Medium Feature)

### Your First Feature (Learning)
- **Phase 0**: 60 min
- **Phase 1**: 90 min
- **Phase 2**: 15 min
- **Phase 3**: 8 hours (includes learning)
- **Phase 4**: 2 hours
- **Phase 5**: 30 min
- **TOTAL**: ~12 hours

### After 5 Features (Proficient)
- **Phase 0**: 30 min
- **Phase 1**: 45 min
- **Phase 2**: 10 min
- **Phase 3**: 4 hours
- **Phase 4**: 1 hour
- **Phase 5**: 20 min
- **TOTAL**: ~6 hours

**Why it gets faster**: Muscle memory, pattern recognition, tool familiarity

---

## ðŸ“ˆ Progressive Usage Guide

### Features 1-2 (Read Everything)
- Read full SOP sections
- Use every prompt exactly
- Check every checkpoint
- Pair with senior developer
- Ask questions freely

### Features 3-5 (Reference Mode)
- Jump to relevant sections
- Adapt prompts to your style
- Trust judgment on checkpoints
- Consult Implementation Guide less often

### Features 6+ (Expert Mode)
- Use this Quick Ref primarily
- Consult SOP for edge cases
- Customize prompts heavily
- Mentor newer developers

---

## âš¡ What Makes AI Development Different

| Aspect | Traditional | AI-Assisted |
|--------|-------------|-------------|
| **Speed** | Steady | Fast generation |
| **Specs** | Nice to have | **Mandatory** |
| **Testing** | When convenient | **TDD required** |
| **Review** | 1 human | Multiple AI + humans |
| **Docs** | After code | During code |

**Key insight**: AI is fast at generating â†’ Invest time in specs & tests upfront

---

## ðŸ†˜ If You're Stuck

**2-minute rule**: Try to solve yourself for 2 minutes
**15-minute rule**: If stuck > 15 min, ask for help
**2-hour rule**: If stuck > 2 hours, escalate to senior/lead

**Where to get help:**
1. Check constitution & implementation guide
2. Ask different AI model for perspective
3. Post in team chat
4. Pair with senior developer
5. Create ADR if architecture question

---

## ðŸš© Red Flags (Call for Help)

- Writing code before tests
- Same AI generating + reviewing
- Coverage < 80% "just this once"
- "I'll add tests later"
- Constitution violation "necessary for this feature"
- Skipping Docker testing
- Stuck on same issue > 2 hours

**If you see these in your workflow â†’ PAUSE and get help**

---

## ðŸŽ¯ Success Metrics (After 5 Features, You Should...)

- Generate specs in < 1 hour
- Write tests before implementation naturally
- Know constitution patterns by memory
- Use different AI for reviews automatically
- Complete features in ~6 hours
- Rarely fail checkpoints
- Help newer developers with questions

---

## ðŸ’¡ Remember

**This process exists to help you ship better code faster.**

- Specs prevent rework
- Tests catch bugs early
- Multiple AIs catch more issues
- Constitution keeps architecture consistent
- Upfront investment saves debugging time

**After a few features, this becomes natural muscle memory.**

---

**Full SOP**: See your project's documentation folder
**Questions**: Ask in team chat
**Feedback**: Create issue with suggested improvements

---

ðŸš€ **Happy coding! Remember: Test first, review with different AI, trust your instincts.**
