# Prompt Library (Copy/Paste)

**Sources of truth**:
- `AI-DEVELOPMENT-SOP-v1.4.0-FINAL.md` → “Common Prompts Reference”
- `AI-DEV-QUICK-REFERENCE-v1.4.0-FINAL.md` → “Common Prompts (Copy-Paste Ready)” + other embedded prompts

---

## Research / Pre-Implementation

### Codebase analysis
```
"Analyze the codebase for [feature] and create a pre-implementation document that covers:
- Related existing code
- Files to modify
- Technical decisions needed
- Risks and dependencies"
```

### Pre-implementation analysis (Phase 0)
```
"Before implementing [STORY-ID], perform a pre-implementation analysis:
1. Scan the codebase for related code
2. Identify files that will need modification
3. Identify dependencies on other features
4. Suggest an implementation approach
5. Flag any constitution concerns

Document findings in your project's session logs or documentation area"
```

---

## Specification

### Specification phase
```
"Generate specification for [STORY-ID]. Ensure all acceptance criteria from the feature specification are included and clear."
```

### Manual spec generation (when Speckit is unavailable)
```
"Create a detailed specification for [STORY-ID]:

User Story: [paste from feature specification]
Acceptance Criteria: [paste from feature specification]

Generate a spec.md that includes:
1. Feature Overview
2. Technical Requirements
3. API Contracts (if applicable)
4. Data Model Changes (if applicable)
5. Test Scenarios
6. Success Criteria

Format as markdown, following industry-standard specification structure"
```

---

## Start a new feature (developer workflow)

```
"I'm starting [STORY-ID]. Read constitution, implementation guide, feature specification.
Perform pre-implementation analysis and identify:
1. Files to create/modify
2. Dependencies
3. Technical decisions needed"
```

---

## TDD Implementation (mandatory)

```
"Implement [task] following TDD:
1. Write the test first at [test file path]
2. Show me the test code
3. Run it (should FAIL/RED)
4. Then implement the code to make it pass (GREEN)
5. Refactor if needed while keeping tests GREEN"
```

---

## Code Review (use a different model)

```
"Review my implementation (generated by [MODEL]) against:
- Constitution: [paste relevant sections]
- Spec: [paste acceptance criteria]

Check for:
1. Constitution violations
2. Security issues
3. Performance problems  
4. Missing error handling
5. Edge cases not tested

Generate detailed review report."
```

---

## Documentation updates

```
"Update documentation for [feature]:
1. README.md - Add new setup steps if any
2. API docs - Update endpoints
3. Configuration - Document new variables

Keep updates concise and accurate to implementation."
```

---

## Quickstart testing guide

```
"Create a quickstart testing guide for [feature]:

Generate a testing guide that includes:
1. **Prerequisites** - What needs to be installed/running
2. **Setup Steps** - How to get the environment ready
3. **Quick Test** - Simplest way to verify it works (< 2 minutes)
4. **Full Test Suite** - How to run all tests
5. **Common Issues** - Troubleshooting guide
6. **Example Usage** - Sample requests/commands with expected output

Make it copy-paste friendly for someone testing this for the first time."
```

---

## Notes

- These prompts are intentionally **generic** so they work across stacks.
- Always include: **(1) spec**, **(2) constitution constraints**, and **(3) test-first requirement**.
- For sensitive repos, ensure your AI tool usage complies with your org’s data policy.
